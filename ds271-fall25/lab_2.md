# Abstract: A Time Series Analysis of the Federal Reserve's Interest Rate Policy Using the Taylor Rule

This research investigates the Federal Reserve's interest rate setting behavior by estimating the basic Taylor Rule, a guideline suggesting interest rate adjustments based on the inflation gap and the output gap. Quarterly U.S. macroeconomic time series data (interest rate, inflation, and output gap) were collected, preprocessed using percentile-based Winsorizing to treat outliers, and differenced as required by unit root tests to achieve stationarity. Estimation of the Taylor Rule via Ordinary Least Squares (OLS) regression yielded an inflation gap coefficient of 1.3675, significantly exceeding the theoretical value of 1, suggesting an historically aggressive policy response to inflation. However, the OLS model diagnostics indicated severe autocorrelation in the residuals, violating a key assumption. Although cointegration was detected between the variables (suggesting a long-run equilibrium relationship), a comparative analysis of the OLS, Error Correction Model (ECM), ARIMA, and Vector Autoregression (VAR) models showed that the Taylor Rule (OLS) and VAR models generalized better to the test data, with the ECM performing the weakest overall in terms of RMSE.

[Check out this report](lab_2.pdf)